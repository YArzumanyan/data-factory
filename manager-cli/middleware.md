# Middleware Pipeline Configuration

## 1. Introduction

This document describes the JSON format used to define data processing pipelines for the Middleware component. The Middleware accepts this JSON configuration, validates it, translates it into a structured RDF graph using standard vocabularies (P-Plan, DCAT, DCTerms, PROV-O) and custom terms (`df:`), and then submits this RDF graph to the backend `metadata-store`.

The JSON format allows users to define:
* The pipeline's overall metadata.
* The **logical variables** (`p-plan:Variable`, `var:` prefix) representing the *flow* of data artifacts within the pipeline (initial inputs, intermediates). Initial input variables *may optionally* be linked to pre-existing Datasets in the `metadata-store`.
* The processing steps (`p-plan:Step`, `step:` prefix).
* The **UUIDs of pre-existing plugins** (`pl:` prefix) used by each step.
* The dependencies between steps.
* How steps consume and produce the internal logical variables (`var:`).

**Key Constraints & Concepts:**
* **Plugins** referenced by `pluginUuid` **must** pre-exist in the `metadata-store`.
* **Existing Datasets** linked via `datasetUuid` (for initial inputs) **must** pre-exist in the `metadata-store`.
* **Pipeline Variables (`var:`):** Represent the data flow *within* the pipeline. They are typed **only** as `p-plan:Variable`. They are **not** typed `dcat:Dataset`.
* **Output Datasets (`ds:`):** Represent the final, persistent outputs intended for reuse. They are **auto-generated by the Middleware** for each terminal variable produced by the pipeline. They are typed **only** as `dcat:Dataset` and use the `ds:` prefix. They are **not** typed `p-plan:Variable`.
* **Linking:**
    * Initial `var:` inputs can link to existing `ds:` datasets via `prov:specializationOf`.
    * Steps consume/produce `var:` variables via `p-plan:hasInputVar`/`p-plan:isOutputVarOf`.
    * Auto-generated output `ds:` datasets link to the final `var:` variable they derive from via `prov:wasDerivedFrom` and to the generating pipeline (`pipe:`) via `prov:wasGeneratedBy`.
* **Reusability:** Only the auto-generated output datasets (`ds:<uuid>`) are intended as stable inputs for subsequent pipelines. Internal/intermediate variables (`var:<uuid>`) should generally not be referenced directly by other pipelines.

## 2. JSON Structure Overview

The pipeline configuration is a single JSON object with the following main properties:

* **`title` (String, Required):** A human-readable name for the pipeline.
* **`description` (String, Optional):** A longer description of the pipeline's purpose.
* **`variables` (Array, Required):** A list defining the logical variables (`p-plan:Variable`) used or produced within the pipeline context.
* **`steps` (Array, Required):** A list defining the individual processing steps.

## 3. Detailed Field Descriptions

### 3.1 Root Object

| Field         | Type   | Required | Description                         |
| :------------ | :----- | :------- | :---------------------------------- |
| `title`       | String | Yes      | The main title of the pipeline.     |
| `description` | String | No       | Optional description for pipeline.  |
| `variables`   | Array  | Yes      | An array of Variable Objects.       |
| `steps`       | Array  | Yes      | An array of Step Objects.           |

### 3.2 Variable Object (within `variables` array)

Represents a `p-plan:Variable` used for data flow within the pipeline.

| Field         | Type   | Required | Description                                                                                                                                    |
| :------------ | :----- | :------- | :--------------------------------------------------------------------------------------------------------------------------------------------- |
| `id`          | String | Yes      | Unique logical identifier for this variable *within this JSON file*. Used in step `inputs`/`outputs`.                                          |
| `title`       | String | Yes      | Human-readable title for this variable.                                                                                                        |
| `datasetUuid` | String | No       | **Optional UUID** of a pre-existing `dcat:Dataset` in the `metadata-store`. **Use only for variables representing initial inputs.** Must exist if provided. |

### 3.3 Step Object (within `steps` array)

Represents a single processing step in the pipeline.

| Field        | Type   | Required            | Description                                                                                           |
| :----------- | :----- | :------------------ | :---------------------------------------------------------------------------------------------------- |
| `id`         | String | Yes                 | Unique logical identifier for this step *within this JSON file*. Used in `precededBy`.                  |
| `title`      | String | Yes                 | Human-readable title for this step.                                                                   |
| `pluginUuid` | String | Yes                 | **UUID** of the registered `df:Plugin`. **Must** exist in `metadata-store`.                           |
| `inputs`     | Array  | No (but recommended) | Array of logical `id` strings (from `variables` array) identifying input variables (`var:`) for this step. |
| `outputs`    | Array  | No (but recommended) | Array of logical `id` strings (from `variables` array) identifying output variables (`var:`) for this step. |
| `precededBy` | Array  | No                  | Array of logical step `id` strings (from this JSON) defining execution order dependencies.             |

## 4. JSON Schema

*(Schema remains the same as V3/V4, as the interpretation change happens during RDF generation)*

```json
{
  "$schema": "[http://json-schema.org/draft-07/schema#](http://json-schema.org/draft-07/schema#)",
  "title": "Pipeline Configuration Schema (V5)",
  "description": "Defines the structure for configuring a data processing pipeline, including internal variables (p-plan:Variable), references to pre-existing plugins, and optionally initial input datasets. Final output datasets (dcat:Dataset) are generated automatically.",
  "type": "object",
  "properties": {
    "title": {
      "description": "Human-readable title for the pipeline.",
      "type": "string"
    },
    "description": {
      "description": "Optional longer description of the pipeline's purpose.",
      "type": "string"
    },
    "variables": {
      "description": "List of logical variables (p-plan:Variable) representing data flow within the pipeline scope.",
      "type": "array",
      "minItems": 1,
      "items": {
        "$ref": "#/definitions/variable"
      }
    },
    "steps": {
      "description": "List of processing steps in the pipeline.",
      "type": "array",
      "minItems": 1,
      "items": {
        "$ref": "#/definitions/step"
      }
    }
  },
  "required": [
    "title",
    "variables",
    "steps"
  ],
  "definitions": {
    "variable": {
      "type": "object",
      "properties": {
        "id": {
          "description": "Unique logical identifier for the variable within this pipeline configuration.",
          "type": "string",
          "pattern": "^[a-zA-Z0-9_-]+$"
        },
        "title": {
          "description": "Human-readable title for the variable.",
          "type": "string"
        },
        "datasetUuid": {
          "description": "Optional UUID of a pre-existing dcat:Dataset this variable represents (typically for initial inputs). Must exist in the metadata store if provided.",
          "type": "string",
          "format": "uuid"
        }
      },
      "required": [
        "id",
        "title"
      ]
    },
    "step": {
      "type": "object",
      "properties": {
        "id": {
          "description": "Unique logical identifier for the step within this pipeline configuration.",
          "type": "string",
          "pattern": "^[a-zA-Z0-9_-]+$"
        },
        "title": {
          "description": "Human-readable title for the step.",
          "type": "string"
        },
        "pluginUuid": {
          "description": "The UUID of the registered plugin (df:Plugin) used by this step. Must exist in the metadata store.",
          "type": "string",
          "format": "uuid"
        },
        "inputs": {
          "description": "List of logical variable IDs (from the 'variables' array) that are inputs to this step.",
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "outputs": {
          "description": "List of logical variable IDs (from the 'variables' array) that are outputs of this step.",
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "precededBy": {
          "description": "Optional list of logical step IDs (from this JSON) that must complete before this step starts.",
          "type": "array",
          "items": {
            "type": "string"
          }
        }
      },
      "required": [
        "id",
        "title",
        "pluginUuid"
      ]
    }
  }
}
```

## 5. JSON Parsing and RDF Conversion Process

Upon receiving a JSON payload validated against the schema, the Middleware performs the following conversion to generate an RDF graph (Turtle syntax) for the `metadata-store`:

1.  **UUID Generation & Mapping:**
    * Generate a unique UUID (`<uuidP>`) for the pipeline. Define its RDF identifier: `pipelineId = pipe:<uuidP>`.
    * Initialize two maps:
        * `variableIdMap`: Maps JSON variable `id` -> RDF identifier **`var:<uuidV>`**. *(Using `var:` prefix)*
        * `stepIdMap`: Maps JSON step `id` -> RDF identifier `step:<uuidS>`.

2.  **RDF Model Initialization:**
    * Create an empty Apache Jena `Model`.
    * Define necessary prefixes (e.g., `rdf:`, `dcterms:`, `p-plan:`, `dcat:`, `prov:`, `df:`, `pipe:`, `step:`, `var:`, `ds:`, `pl:`). Define base URIs for identifiers (e.g., `urn:pipe:`, `urn:var:`, `urn:ds:`).

3.  **Create Pipeline Resource:**
    * Add triples for the pipeline plan: `<pipelineId> rdf:type p-plan:Plan ; dcterms:title "..." ; dcterms:description "..." .`

4.  **Process Variables:**
    * Iterate through the `variables` JSON array.
    * For each `variable` object:
        * Generate a variable UUID (`<uuidV>`). Define RDF identifier: **`variableIdRdf = var:<uuidV>`**. *(Using `var:` prefix)*
        * Store the mapping: `variableIdMap.put(jsonVariableId, variableIdRdf)`.
        * Add core variable triples (**only `p-plan:Variable` type**):
            ```turtle
            <variableIdRdf> rdf:type p-plan:Variable ;
                           dcterms:title "..." ;
                           p-plan:isVariableOfPlan <pipelineId> .
            ```
        * **Handle Optional Input Dataset Mapping:** If `datasetUuid` (`<existingUuidD>`) is present:
            * Define the existing dataset's RDF identifier: `existingDatasetIdRdf = ds:<existingUuidD>`.
            * **Validation:** Query the `metadata-store` (e.g., `GET /api/v1/resources/{existingUuidD}`) to verify that a resource with the UUID `<existingUuidD>` exists. **Fail (e.g., 422 Unprocessable Entity)** if it does not.
            * Add the linking triple: `<variableIdRdf> prov:specializationOf <existingDatasetIdRdf> .`

5.  **Process Steps (First Pass - Create Steps & Links):**
    * Iterate through the `steps` JSON array.
    * For each `step` object:
        * Generate a step UUID (`<uuidS>`). Define RDF identifier: `stepIdRdf = step:<uuidS>`.
        * Store the mapping: `stepIdMap.put(jsonStepId, stepIdRdf)`.
        * Add core step triples: `<stepIdRdf> rdf:type p-plan:Step ; dcterms:title "..." ; p-plan:isStepOfPlan <pipelineId> .`
        * **Link Plugin & Validate:**
            * Get `pluginUuid` from JSON. Define plugin RDF identifier: `pluginIdRdf = pl:<pluginUuid>`.
            * **Validation:** Query `metadata-store` (e.g., `GET /api/v1/resources/{pluginUuid}`) to verify resource exists. **Fail (422)** if not.
            * Add triple: `<stepIdRdf> df:usesPlugin <pluginIdRdf> .`
        * **Link Inputs:** For each `variableId` in the `inputs` array:
            * Look up the corresponding variable RDF identifier (`variableIdRdf` starting with `var:`) from `variableIdMap`.
            * Add triple: `<stepIdRdf> p-plan:hasInputVar <variableIdRdf> .`
        * **Link Outputs:** For each `variableId` in the `outputs` array:
            * Look up the corresponding variable RDF identifier (`variableIdRdf` starting with `var:`) from `variableIdMap`.
            * Add triple: `<stepIdRdf> p-plan:isOutputVarOf <variableIdRdf> .`

6.  **Process Steps (Second Pass - Add Dependencies):**
    * Iterate through the `steps` JSON array again.
    * Add `p-plan:isPrecededBy` triples based on the `precededBy` array, using `stepIdMap`.

7.  **Identify Final Variables & Generate Output Datasets:**
    * Analyze the generated step/variable links within the current pipeline's RDF `Model`.
    * Identify all `var:<uuidV>` resources that are produced by a step (`p-plan:isOutputVarOf`) but are **not** consumed as input by any *other* step (`p-plan:hasInputVar`) within this same pipeline definition. These are the "terminal" variables.
    * For each identified terminal variable (`<terminalVarIdRdf>`):
        * Generate a new UUID (`<outputDsUuid>`). Define RDF identifier: `outputDatasetIdRdf = ds:<outputDsUuid>`.
        * Retrieve the title from the terminal variable (e.g., using `dcterms:title`).
        * Add triples for the auto-generated output dataset placeholder:
            ```turtle
            <outputDatasetIdRdf> rdf:type dcat:Dataset ;
                                 dcterms:title "<Title from variable> (Output)" ; # Or other naming convention
                                 prov:wasDerivedFrom <terminalVarIdRdf> ;
                                 prov:wasGeneratedBy <pipelineId> .
            ```

8.  **Final RDF Validation:**
    * Perform any final structural validation on the generated `Model` if needed.

9.  **Submission:**
    * Serialize the final Jena `Model` (containing the pipeline, steps, variables, *and* the auto-generated output datasets) to Turtle syntax.
    * POST the Turtle data to the `metadata-store`'s `/api/v1/pipelines` endpoint.

## 6. Example

**Input JSON:** *(Same as V3/V4)*

```json
{
  "title": "Validation & Cleansing Pipeline v3",
  "variables": [
    {
      "id": "raw-data-var",
      "title": "Initial Raw Input",
      "datasetUuid": "dddddddd-dddd-dddd-dddd-dataset001" // Links to existing input dataset
    },
    {
      "id": "validated-data-var",
      "title": "Intermediate Validated Data" // No datasetUuid - this is internal
    },
    {
      "id": "cleansed-output-var",
      "title": "Final Cleansed Output Var" // No datasetUuid - this is final internal variable
    }
  ],
  "steps": [
    {
      "id": "validate-step",
      "title": "Validate Raw",
      "pluginUuid": "pppppppp-pppp-pppp-pppp-plugin001",
      "inputs":  ["raw-data-var"],
      "outputs": ["validated-data-var"]
    },
    {
      "id": "cleanse-step",
      "title": "Cleanse Data",
      "pluginUuid": "qqqqqqqq-qqqq-qqqq-qqqq-plugin002",
      "inputs": ["validated-data-var"],
      "outputs": ["cleansed-output-var"], // This variable is terminal
      "precededBy": ["validate-step"]
    }
  ]
}
```

**Conceptual Output Turtle (Illustrative UUIDs, `var:` prefix, Auto-generated Output `ds:`):**

```turtle
@prefix rdf: [http://www.w3.org/1999/02/22-rdf-syntax-ns#](http://www.w3.org/1999/02/22-rdf-syntax-ns#) .
@prefix dcterms: [http://purl.org/dc/terms/](http://purl.org/dc/terms/) .
@prefix p-plan: [http://purl.org/net/p-plan#](http://purl.org/net/p-plan#) .
@prefix dcat: [http://www.w3.org/ns/dcat#](http://www.w3.org/ns/dcat#) .
@prefix prov: [http://www.w3.org/ns/prov#](http://www.w3.org/ns/prov#) .
@prefix df: [http://example.org/ns/df#](http://example.org/ns/df#) .
# Define prefixes used for identifiers (using URNs as an example base)
@prefix pipe: <urn:pipe:> .
@prefix step: <urn:step:> .
@prefix var: <urn:var:> . # Prefix for variables
@prefix ds: <urn:ds:> .  # Prefix for datasets (pre-existing and auto-generated outputs)
@prefix pl: <urn:pl:> .

# Pipeline (Generated UUID: uuidP-555)
pipe:uuidP-555 rdf:type p-plan:Plan ;
  dcterms:title "Validation & Cleansing Pipeline v3" .

# Variables (Generated UUIDs: uuidV-PPP, uuidV-QQQ, uuidV-RRR)
var:uuidV-PPP rdf:type p-plan:Variable ; # Corresponds to "raw-data-var"
  dcterms:title "Initial Raw Input" ;
  p-plan:isVariableOfPlan pipe:uuidP-555 ;
  prov:specializationOf ds:dddddddd-dddd-dddd-dddd-dataset001 . # Link to existing dataset

var:uuidV-QQQ rdf:type p-plan:Variable ; # Corresponds to "validated-data-var"
  dcterms:title "Intermediate Validated Data" ;
  p-plan:isVariableOfPlan pipe:uuidP-555 .

var:uuidV-RRR rdf:type p-plan:Variable ; # Corresponds to "cleansed-output-var"
  dcterms:title "Final Cleansed Output Var" ;
  p-plan:isVariableOfPlan pipe:uuidP-555 .

# Steps (Generated UUIDs: uuidS-S9, uuidS-S10)
step:uuidS-S9 rdf:type p-plan:Step ; # Corresponds to "validate-step"
  dcterms:title "Validate Raw" ;
  p-plan:isStepOfPlan pipe:uuidP-555 ;
  df:usesPlugin pl:pppppppp-pppp-pppp-pppp-plugin001 ;
  p-plan:hasInputVar var:uuidV-PPP ;
  p-plan:isOutputVarOf var:uuidV-QQQ .

step:uuidS-S10 rdf:type p-plan:Step ; # Corresponds to "cleanse-step"
  dcterms:title "Cleanse Data" ;
  p-plan:isStepOfPlan pipe:uuidP-555 ;
  df:usesPlugin pl:qqqqqqqq-qqqq-qqqq-qqqq-plugin002 ;
  p-plan:hasInputVar var:uuidV-QQQ ;
  p-plan:isOutputVarOf var:uuidV-RRR ; # Produces the terminal variable
  p-plan:isPrecededBy step:uuidS-S9 .

# Auto-generated Output Dataset Placeholder (Generated UUID: uuidDS-OUT-XYZ)
ds:uuidDS-OUT-XYZ rdf:type dcat:Dataset ;
  dcterms:title "Final Cleansed Output Var (Output)" ; # Title derived from variable
  prov:wasDerivedFrom var:uuidV-RRR ; # Link to the terminal variable
  prov:wasGeneratedBy pipe:uuidP-555 . # Link to the pipeline

```

## 7. Target RDF Vocabularies

The generated RDF graph primarily uses the following vocabularies:

* **P-Plan:** For describing the workflow structure (Plan, Step, Variable, linking properties). Variables are typed `p-plan:Variable`.
* **DCTerms:** For metadata like titles and descriptions.
* **`df:` (Custom):** For linking steps to plugins (`df:usesPlugin`).
* **PROV-O:** For linking initial input variables to pre-existing datasets (`prov:specializationOf`), linking auto-generated output datasets to terminal variables (`prov:wasDerivedFrom`), and linking output datasets to the generating pipeline (`prov:wasGeneratedBy`).
* **DCAT:** Used **only** to type the auto-generated final output datasets (`dcat:Dataset`). Pipeline variables (`var:`) are **not** typed `dcat:Dataset`.
* **RDF/RDFS:** For basic typing (`rdf:type`).
* **XSD:** For literal datatypes (optional).